{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2817e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\ornl_dsf\\webste background\\davis\\cascadia-earthquake-viewer\\data_analysis\n",
      "\n",
      "Dataset files found:\n",
      "  ‚úì dataset_field_comparison.xlsx\n",
      "  ‚úì ds01.xlsx\n",
      "  ‚úì filt_events_duplicates_removed_v2.csv\n",
      "  ‚úì hypreloc_fix5_4pr_all.reloc\n",
      "  ‚úì LFEcat_MTJ_2018-2024.csv\n",
      "  ‚úì lfe_family_locations.txt\n",
      "  ‚úì Merrill_et_al_REST_Catalogue.dat\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Verify we're in the correct directory\n",
    "print(\"Current working directory:\", Path.cwd())\n",
    "print(\"\\nDataset files found:\")\n",
    "for file in Path('.').glob('*'):\n",
    "    if file.suffix in ['.csv', '.dat', '.reloc', '.xlsx', '.txt']:\n",
    "        print(f\"  ‚úì {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b3aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_25192\\4279887936.py:2: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df0 = pd.read_csv('filt_events_duplicates_removed_v2.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dataset 0: Brenton et al. (Current Production)\n",
      "================================================================================\n",
      "Total events: 279,148\n",
      "\n",
      "Columns (28):\n",
      "['Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'evid', 'lon', 'lat', 'dep', 'x', 'y', 'z', 'ot', 'ex', 'ey', 'ez', 'cxy', 'cxz', 'cyz', 'rms', 'gap', 'nsta', 'nphases', 'max_err', 'newdep', 'region', 'dtime', 'datetime', 'duplicate_cluster_id']\n",
      "\n",
      "Sample data:\n",
      "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0               evid  \\\n",
      "0             0            28            44          44  W1.20020110205424   \n",
      "1             0            53            77          77  W1.20020117000943   \n",
      "2             0            84           128         128  W1.20020124202006   \n",
      "3             0           170           338         338  W1.20020213133054   \n",
      "4             0        197193           269         269  W2.20020214223052   \n",
      "\n",
      "          lon        lat        dep         x         y  ...       rms  \\\n",
      "0 -120.523958  46.633877  10.852295  131.9300  -94.9579  ...  0.316607   \n",
      "1 -120.997560  46.691615  13.317627   95.6314  -89.2216  ...  0.267555   \n",
      "2 -120.062993  46.391777   9.083057  167.9060 -121.0180  ...  0.277260   \n",
      "3 -121.074256  46.711204   8.406299   89.7429  -87.1323  ...  0.153711   \n",
      "4 -119.764601  46.233089   8.154932  191.3690  151.3520  ...  0.100810   \n",
      "\n",
      "        gap  nsta  nphases   max_err     newdep  region     dtime  \\\n",
      "0   71.6058    18       26  0.825561  10.485289      W1  0.000792   \n",
      "1   72.6629    18       31  0.829980  11.995155      W1  0.179298   \n",
      "2  107.0860    14       22  0.893463   8.756772      W1  0.795294   \n",
      "3  124.3660    11       16  1.056362   6.879392      W1  0.748807   \n",
      "4   74.2806     8       14  1.634549   7.928546      W2  0.218342   \n",
      "\n",
      "                     datetime  duplicate_cluster_id  \n",
      "0  2002-01-10 20:54:24.971769                   2.0  \n",
      "1  2002-01-17 00:09:44.265329                   3.0  \n",
      "2  2002-01-24 20:20:07.083086                   6.0  \n",
      "3  2002-02-13 13:30:54.888013                  11.0  \n",
      "4  2002-02-14 22:30:53.015405                  12.0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "Data types:\n",
      "Unnamed: 0.3              int64\n",
      "Unnamed: 0.2              int64\n",
      "Unnamed: 0.1              int64\n",
      "Unnamed: 0                int64\n",
      "evid                     object\n",
      "lon                     float64\n",
      "lat                     float64\n",
      "dep                     float64\n",
      "x                       float64\n",
      "y                       float64\n",
      "z                       float64\n",
      "ot                      float64\n",
      "ex                      float64\n",
      "ey                      float64\n",
      "ez                      float64\n",
      "cxy                     float64\n",
      "cxz                     float64\n",
      "cyz                     float64\n",
      "rms                     float64\n",
      "gap                     float64\n",
      "nsta                      int64\n",
      "nphases                   int64\n",
      "max_err                 float64\n",
      "newdep                  float64\n",
      "region                   object\n",
      "dtime                   float64\n",
      "datetime                 object\n",
      "duplicate_cluster_id    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Dataset 0: Brenton et al. (Current production catalog)\n",
    "df0 = pd.read_csv('filt_events_duplicates_removed_v2.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Dataset 0: Brenton et al. (Current Production)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total events: {len(df0):,}\")\n",
    "print(f\"\\nColumns ({len(df0.columns)}):\")\n",
    "print(list(df0.columns))\n",
    "print(f\"\\nSample data:\")\n",
    "print(df0.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df0.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea8c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset 1: Littel et al. 2024\n",
      "================================================================================\n",
      "Total events: 18,442\n",
      "\n",
      "Columns (24):\n",
      "['ID', 'LAT', 'LON', 'DEPTH', 'X', 'Y', 'Z', 'EX', 'EY', 'EZ', 'YR', 'MO', 'DY', 'HR', 'MI', 'SC', 'MAG', 'NCCP', 'NCCS', 'NCTP', 'NCTS', 'RCC', 'RCT', 'CID']\n",
      "\n",
      "Sample data:\n",
      "       ID        LAT          LON  DEPTH        X          Y        Z    EX  \\\n",
      "0      ID        LAT          LON  DEPTH        X          Y        Z    EX   \n",
      "1  107951  50.889807  -130.609530  6.768  66809.0  -114906.3   1768.8  91.2   \n",
      "2      70  51.537431  -130.915584  2.481  45192.6   -42848.3  -2518.4  35.8   \n",
      "3     132  50.828312  -130.651294  4.657  63946.2  -121748.8   -342.5  55.2   \n",
      "4     136  50.832279  -130.657471  3.830  63513.3  -121307.3  -1169.3  68.6   \n",
      "\n",
      "      EY    EZ  ...  MI      SC  MAG  NCCP  NCCS  NCTP  NCTS     RCC     RCT  \\\n",
      "0     EY    EZ  ...  MI      SC  MAG  NCCP  NCCS  NCTP  NCTS     RCC     RCT   \n",
      "1  110.4  79.6  ...  28  46.280  0.0     1     0    14     6   0.000   0.117   \n",
      "2   63.3  18.6  ...   4  15.640  0.0     3     0     0     0   0.002  -9.000   \n",
      "3   95.3  34.1  ...  55  14.920  0.0     0     0    40    19  -9.000   0.104   \n",
      "4  106.5  31.1  ...  47  22.690  0.0     2     0    37    17   0.020   0.118   \n",
      "\n",
      "   CID  \n",
      "0  CID  \n",
      "1    1  \n",
      "2    1  \n",
      "3    1  \n",
      "4    1  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dataset 1: Littel et al. 2024 - hypoDD reloc format\n",
    "# Based on manual: ID, LAT, LON, DEPTH, X, Y, Z, EX, EY, EZ, YR, MO, DY, HR, MI, SC, MAG, etc.\n",
    "\n",
    "df1 = pd.read_csv('hypreloc_fix5_4pr_all.reloc', sep=r'\\s+', header=None,\n",
    "                   names=['ID', 'LAT', 'LON', 'DEPTH', 'X', 'Y', 'Z', 'EX', 'EY', 'EZ', \n",
    "                          'YR', 'MO', 'DY', 'HR', 'MI', 'SC', 'MAG', 'NCCP', 'NCCS', \n",
    "                          'NCTP', 'NCTS', 'RCC', 'RCT', 'CID'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset 1: Littel et al. 2024\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total events: {len(df1):,}\")\n",
    "print(f\"\\nColumns ({len(df1.columns)}):\")\n",
    "print(list(df1.columns))\n",
    "print(f\"\\nSample data:\")\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2b95d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_25192\\698148926.py:2: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('Merrill_et_al_REST_Catalogue.dat',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset 2: Merrill et al.\n",
      "================================================================================\n",
      "Total events: 108,676\n",
      "\n",
      "Columns (14):\n",
      "['eventID', 'lat', 'lon', 'depth', 'year', 'month', 'day', 'hour', 'minute', 'second', 'x_error', 'z_error', 'label', 'grade']\n",
      "\n",
      "Sample data:\n",
      "                  eventID     lat    lon   depth  year  month      day  \\\n",
      "eventID, lat,        lon,  depth,  year,  month,  day,  hour,  minute,   \n",
      "5        48.623  -125.894  23.008   2000       1     1     17       37   \n",
      "7        48.849  -123.541  12.351   2000       1     2      0       22   \n",
      "20       48.604  -123.105  13.459   2000       1     3      4       40   \n",
      "44       48.804  -125.148  14.513   2000       1     9     21       48   \n",
      "\n",
      "                    hour   minute second  x_error z_error   label  grade  \n",
      "eventID, lat,    second,  x-error  (km),  z-error   (km),  label,  grade  \n",
      "5        48.623     7.27     1.16   2.06        3       S     NaN    NaN  \n",
      "7        48.849    45.91     0.71   1.54        1       S     NaN    NaN  \n",
      "20       48.604    17.88     0.61   2.99        1       S     NaN    NaN  \n",
      "44       48.804    50.58     0.55   2.25        3       S     NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# Dataset 2: Merrill et al. - REST catalog\n",
    "df2 = pd.read_csv('Merrill_et_al_REST_Catalogue.dat', \n",
    "                   sep=r'\\s+',\n",
    "                   header=None,\n",
    "                   names=['eventID', 'lat', 'lon', 'depth', 'year', 'month', 'day', \n",
    "                          'hour', 'minute', 'second', 'x_error', 'z_error', 'label', 'grade'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset 2: Merrill et al.\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total events: {len(df2):,}\")\n",
    "print(f\"\\nColumns ({len(df2.columns)}):\")\n",
    "print(list(df2.columns))\n",
    "print(f\"\\nSample data:\")\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07ac9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset 3: Morton et al. 2023\n",
      "================================================================================\n",
      "Total events: 5,282\n",
      "\n",
      "Columns (23):\n",
      "['CI YEAR', 'TSTRING', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'LAT', 'LON', 'DEPTH', 'Md', 'Num P&S with weights > 0.1', 'max az gap', 'dist to nearest stn', 'tt RMS', 'ERH', 'ERZ', 'STRIKE', 'DIP', 'RAKE', 'PLATE DESIGNATION', 'TEMPLATE EVENT?']\n",
      "\n",
      "Sample data:\n",
      "   CI YEAR       TSTRING  YEAR  MONTH  DAY  HOUR  MINUTE  SECOND        LAT  \\\n",
      "0        1  2.011073e+13  2011      7   26     1       2    7.37  47.321667   \n",
      "1        1  2.011073e+13  2011      7   26     1       2    7.72  44.288833   \n",
      "2        1  2.011073e+13  2011      7   26     1       2    8.56  44.301667   \n",
      "3        1  2.011073e+13  2011      7   26     7      31    2.17  48.263500   \n",
      "4        1  2.011073e+13  2011      7   26     9      50   27.63  48.303167   \n",
      "\n",
      "          LON  ...  max az gap  dist to nearest stn  tt RMS   ERH   ERZ  \\\n",
      "0 -123.270833  ...         166                 27.4    0.19   0.8   1.2   \n",
      "1 -124.334000  ...         332                163.8    0.06  13.1   3.2   \n",
      "2 -124.318000  ...         316                131.1    0.50  35.4  22.2   \n",
      "3 -124.929833  ...         205                 44.4    0.77   3.5   6.4   \n",
      "4 -124.915667  ...         199                 46.1    0.94   4.0   6.9   \n",
      "\n",
      "   STRIKE  DIP  RAKE  PLATE DESIGNATION  TEMPLATE EVENT?  \n",
      "0     NaN  NaN   NaN          Interface          Catalog  \n",
      "1     NaN  NaN   NaN        Upper Plate              NaN  \n",
      "2     NaN  NaN   NaN        Upper Plate              NaN  \n",
      "3     NaN  NaN   NaN        Upper Plate              NaN  \n",
      "4     NaN  NaN   NaN        Upper Plate                T  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dataset 3: Morton et al. 2023 - Excel file\n",
    "df3 = pd.read_excel('ds01.xlsx')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset 3: Morton et al. 2023\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total events: {len(df3):,}\")\n",
    "print(f\"\\nColumns ({len(df3.columns)}):\")\n",
    "print(list(df3.columns))\n",
    "print(f\"\\nSample data:\")\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e142eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset 4: Shelly et al. 2025\n",
      "================================================================================\n",
      "Total events: 61,441\n",
      "\n",
      "Columns (17):\n",
      "['%year', 'month', 'day', 'hour', 'minute', 'second', 's_of_day', 'ccsum', 'thresh', 'cc_div_thr', 'meancc', 'nchan', 'ID', 'idnum', 'latitude', 'longitude', 'depth']\n",
      "\n",
      "Sample data:\n",
      "   %year  month  day  hour  minute  second  s_of_day  ccsum  thresh  \\\n",
      "0   2018      1    1     1      26   14.88   5174.88   4.00    2.68   \n",
      "1   2018      1    1     1      27    3.46   5223.46   3.32    2.68   \n",
      "2   2018      1    1    22      23   56.98  80636.98   4.94    2.77   \n",
      "3   2018      1    1    22      24   37.13  80677.13   3.76    2.77   \n",
      "4   2018      1    1    22      24   46.79  80686.79   3.91    2.76   \n",
      "\n",
      "   cc_div_thr  meancc  nchan            ID  idnum   latitude   longitude  \\\n",
      "0        1.49   0.222     18  22111112742s     27  40.036267 -123.532186   \n",
      "1        1.24   0.184     18  22111112742s     27  40.036267 -123.532186   \n",
      "2        1.78   0.329     15  22090836511s      6  40.063208 -123.593563   \n",
      "3        1.36   0.250     15  22090836511s      6  40.063208 -123.593563   \n",
      "4        1.42   0.260     15  22091338816s      1  40.063867 -123.597624   \n",
      "\n",
      "    depth  \n",
      "0  25.939  \n",
      "1  25.939  \n",
      "2  28.223  \n",
      "3  28.223  \n",
      "4  28.189  \n"
     ]
    }
   ],
   "source": [
    "# Dataset 4: Shelly et al. 2025 - LFE catalog\n",
    "df4_events = pd.read_csv('LFEcat_MTJ_2018-2024.csv')\n",
    "\n",
    "# Family locations\n",
    "family_locs = pd.DataFrame([\n",
    "    [1, 40.063867, -123.597624, 28.189],\n",
    "    [2, 40.069832, -123.632715, 27.623],\n",
    "    [3, 40.036072, -123.684090, 22.192],\n",
    "    [4, 40.070557, -123.619751, 28.199],\n",
    "    [5, 40.063883, -123.593148, 28.578],\n",
    "    [6, 40.063208, -123.593563, 28.223],\n",
    "    [7, 40.070020, -123.630452, 27.747],\n",
    "    [8, 40.088265, -123.709562, 24.216],\n",
    "    [9, 40.069922, -123.621134, 28.134],\n",
    "    [10, 40.069836, -123.627124, 27.834],\n",
    "    [11, 40.040125, -123.675838, 22.449],\n",
    "    [12, 40.068445, -123.626107, 27.775],\n",
    "    [13, 40.069963, -123.623511, 27.960],\n",
    "    [14, 40.036499, -123.668848, 23.295],\n",
    "    [15, 40.069820, -123.632642, 27.698],\n",
    "    [16, 40.064124, -123.599935, 28.170],\n",
    "    [17, 40.070052, -123.620215, 28.126],\n",
    "    [18, 40.067786, -123.610710, 28.209],\n",
    "    [19, 40.072070, -123.686711, 25.676],\n",
    "    [20, 40.064315, -123.601424, 28.162],\n",
    "    [21, 40.027307, -123.606934, 23.712],\n",
    "    [22, 40.073580, -123.644653, 27.497],\n",
    "    [23, 40.053434, -123.556055, 27.045],\n",
    "    [24, 40.064563, -123.601245, 28.189],\n",
    "    [25, 40.069836, -123.619857, 28.164],\n",
    "    [26, 40.072095, -123.712524, 24.127],\n",
    "    [27, 40.036267, -123.532186, 25.939]\n",
    "], columns=['idnum', 'latitude', 'longitude', 'depth'])\n",
    "\n",
    "# Join events with family locations\n",
    "df4 = df4_events.merge(family_locs, on='idnum', how='left')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset 4: Shelly et al. 2025\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total events: {len(df4):,}\")\n",
    "print(f\"\\nColumns ({len(df4.columns)}):\")\n",
    "print(list(df4.columns))\n",
    "print(f\"\\nSample data:\")\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad7f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIELD COMPARISON ACROSS ALL DATASETS\n",
      "================================================================================\n",
      " Universal Field Dataset0_Brenton Dataset1_Littel Dataset2_Merrill Dataset3_Morton Dataset4_Shelly\n",
      "        Event ID             evid              ID          eventID               ‚ùå     ID (family)\n",
      "        Latitude              lat             LAT              lat             LAT        latitude\n",
      "       Longitude              lon             LON              lon             LON       longitude\n",
      "      Depth (km)           newdep           DEPTH            depth           DEPTH           depth\n",
      "     Origin Time       ot (POSIX)               ‚ùå                ‚ùå               ‚ùå               ‚ùå\n",
      "            Year     ‚ùå (parse ot)              YR             year            YEAR           %year\n",
      "           Month                ‚ùå              MO            month           MONTH           month\n",
      "             Day                ‚ùå              DY              day             DAY             day\n",
      "            Hour                ‚ùå              HR             hour            HOUR            hour\n",
      "          Minute                ‚ùå              MI           minute          MINUTE          minute\n",
      "          Second                ‚ùå              SC           second          SECOND          second\n",
      "       Magnitude                ‚ùå             MAG                ‚ùå              Md               ‚ùå\n",
      "    Num Stations             nsta               ‚ùå                ‚ùå         Num P&S           nchan\n",
      "   Azimuthal Gap              gap               ‚ùå                ‚ùå      max az gap               ‚ùå\n",
      "Horizontal Error          max_err              EX          x_error             ERH               ‚ùå\n",
      "  Vertical Error                ‚ùå              EZ          z_error             ERZ               ‚ùå\n",
      "    RMS Residual              rms               ‚ùå                ‚ùå          tt RMS               ‚ùå\n",
      "\n",
      "================================================================================\n",
      "UNIVERSAL FIELDS (Present in ALL 5 datasets):\n",
      "================================================================================\n",
      "‚úÖ Latitude\n",
      "‚úÖ Longitude\n",
      "‚úÖ Depth (km)\n",
      "\n",
      "================================================================================\n",
      "OPTIONAL FIELDS (Missing in some datasets):\n",
      "================================================================================\n",
      "‚ö†Ô∏è  Event ID (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Origin Time (missing in 4/5 datasets)\n",
      "‚ö†Ô∏è  Year (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Month (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Day (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Hour (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Minute (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Second (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Magnitude (missing in 3/5 datasets)\n",
      "‚ö†Ô∏è  Num Stations (missing in 2/5 datasets)\n",
      "‚ö†Ô∏è  Azimuthal Gap (missing in 3/5 datasets)\n",
      "‚ö†Ô∏è  Horizontal Error (missing in 1/5 datasets)\n",
      "‚ö†Ô∏è  Vertical Error (missing in 2/5 datasets)\n",
      "‚ö†Ô∏è  RMS Residual (missing in 3/5 datasets)\n",
      "\n",
      "‚úÖ Saved comparison table to: dataset_field_comparison.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive field mapping\n",
    "comparison = pd.DataFrame({\n",
    "    'Universal Field': ['Event ID', 'Latitude', 'Longitude', 'Depth (km)', 'Origin Time', \n",
    "                        'Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', \n",
    "                        'Magnitude', 'Num Stations', 'Azimuthal Gap', \n",
    "                        'Horizontal Error', 'Vertical Error', 'RMS Residual'],\n",
    "    \n",
    "    'Dataset0_Brenton': ['evid', 'lat', 'lon', 'newdep', 'ot (POSIX)', \n",
    "                         '‚ùå (parse ot)', '‚ùå', '‚ùå', '‚ùå', '‚ùå', '‚ùå', \n",
    "                         '‚ùå', 'nsta', 'gap', 'max_err', '‚ùå', 'rms'],\n",
    "    \n",
    "    'Dataset1_Littel': ['ID', 'LAT', 'LON', 'DEPTH', '‚ùå', \n",
    "                        'YR', 'MO', 'DY', 'HR', 'MI', 'SC', \n",
    "                        'MAG', '‚ùå', '‚ùå', 'EX', 'EZ', '‚ùå'],\n",
    "    \n",
    "    'Dataset2_Merrill': ['eventID', 'lat', 'lon', 'depth', '‚ùå', \n",
    "                         'year', 'month', 'day', 'hour', 'minute', 'second', \n",
    "                         '‚ùå', '‚ùå', '‚ùå', 'x_error', 'z_error', '‚ùå'],\n",
    "    \n",
    "    'Dataset3_Morton': ['‚ùå', 'LAT', 'LON', 'DEPTH', '‚ùå', \n",
    "                        'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', \n",
    "                        'Md', 'Num P&S', 'max az gap', 'ERH', 'ERZ', 'tt RMS'],\n",
    "    \n",
    "    'Dataset4_Shelly': ['ID (family)', 'latitude', 'longitude', 'depth', '‚ùå', \n",
    "                        '%year', 'month', 'day', 'hour', 'minute', 'second', \n",
    "                        '‚ùå', 'nchan', '‚ùå', '‚ùå', '‚ùå', '‚ùå']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIELD COMPARISON ACROSS ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Count which fields exist in ALL datasets\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNIVERSAL FIELDS (Present in ALL 5 datasets):\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in comparison.iterrows():\n",
    "    has_all = all('‚ùå' not in str(val) for val in row[1:])\n",
    "    if has_all:\n",
    "        print(f\"‚úÖ {row['Universal Field']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIONAL FIELDS (Missing in some datasets):\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in comparison.iterrows():\n",
    "    has_some_missing = any('‚ùå' in str(val) for val in row[1:])\n",
    "    if has_some_missing:\n",
    "        missing_count = sum(1 for val in row[1:] if '‚ùå' in str(val))\n",
    "        print(f\"‚ö†Ô∏è  {row['Universal Field']} (missing in {missing_count}/5 datasets)\")\n",
    "\n",
    "# Save to Excel\n",
    "comparison.to_excel('dataset_field_comparison.xlsx', index=False)\n",
    "print(\"\\n‚úÖ Saved comparison table to: dataset_field_comparison.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "543114ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET SIZES:\n",
      "  ‚Ä¢ Dataset 0 (Brenton):  279,148 events\n",
      "  ‚Ä¢ Dataset 1 (Littel):   18,442 events\n",
      "  ‚Ä¢ Dataset 2 (Merrill):  108,676 events\n",
      "  ‚Ä¢ Dataset 3 (Morton):   5,282 events\n",
      "  ‚Ä¢ Dataset 4 (Shelly):   61,441 events\n",
      "  ‚Ä¢ TOTAL:                472,989 events\n",
      "\n",
      "‚úÖ MANDATORY FIELDS (Must be in unified schema):\n",
      "  These exist in ALL 5 datasets:\n",
      "    1. Latitude\n",
      "    2. Longitude\n",
      "    3. Depth (km)\n",
      "    4. Year\n",
      "    5. Month\n",
      "    6. Day\n",
      "    7. Hour\n",
      "    8. Minute\n",
      "    9. Second\n",
      "\n",
      "‚ö†Ô∏è  PROBLEMATIC FIELDS:\n",
      "  ‚Ä¢ Event ID: Missing in Morton (Dataset 3)\n",
      "  ‚Ä¢ Magnitude: Only in Littel (MAG) and Morton (Md) - missing in 3/5 datasets\n",
      "  ‚Ä¢ Num Stations: Different definitions across datasets\n",
      "  ‚Ä¢ Azimuthal Gap: Only in Brenton and Morton\n",
      "  ‚Ä¢ Error metrics: Different types (horizontal vs x/y, vertical vs z)\n",
      "\n",
      "üéØ RECOMMENDED UNIFIED SCHEMA:\n",
      "  REQUIRED FIELDS (all datasets):\n",
      "    ‚Ä¢ id (generate if missing)\n",
      "    ‚Ä¢ latitude\n",
      "    ‚Ä¢ longitude\n",
      "    ‚Ä¢ depth_km\n",
      "    ‚Ä¢ origin_time (construct from year/month/day/hour/minute/second)\n",
      "    ‚Ä¢ catalog_source (Brenton|Littel|Merrill|Morton|Shelly)\n",
      "\n",
      "  OPTIONAL FIELDS (null if missing):\n",
      "    ‚Ä¢ magnitude\n",
      "    ‚Ä¢ num_stations\n",
      "    ‚Ä¢ azimuthal_gap\n",
      "    ‚Ä¢ horizontal_error_km\n",
      "    ‚Ä¢ vertical_error_km\n",
      "    ‚Ä¢ rms_residual\n",
      "\n",
      "üìã NEXT STEPS:\n",
      "  1. ‚úÖ Push dataset_field_comparison.xlsx\n",
      "  2. Get approval on unified schema\n",
      "  3. Write transformation scripts to convert all 5 datasets to unified format\n",
      "  4. Load unified data into PostGIS database\n",
      "  5. Build catalog dropdown in frontend\n",
      "  6. Remove region-specific filters (W1/W2/E1/E2)\n",
      "  7. Add universal filters: lat/lon bounds, depth, date range, magnitude (optional)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä DATASET SIZES:\")\n",
    "print(f\"  ‚Ä¢ Dataset 0 (Brenton):  {len(df0):,} events\")\n",
    "print(f\"  ‚Ä¢ Dataset 1 (Littel):   {len(df1):,} events\")\n",
    "print(f\"  ‚Ä¢ Dataset 2 (Merrill):  {len(df2):,} events\")\n",
    "print(f\"  ‚Ä¢ Dataset 3 (Morton):   {len(df3):,} events\")\n",
    "print(f\"  ‚Ä¢ Dataset 4 (Shelly):   {len(df4):,} events\")\n",
    "print(f\"  ‚Ä¢ TOTAL:                {len(df0)+len(df1)+len(df2)+len(df3)+len(df4):,} events\")\n",
    "\n",
    "print(\"\\n‚úÖ MANDATORY FIELDS (Must be in unified schema):\")\n",
    "print(\"  These exist in ALL 5 datasets:\")\n",
    "print(\"    1. Latitude\")\n",
    "print(\"    2. Longitude\")\n",
    "print(\"    3. Depth (km)\")\n",
    "print(\"    4. Year\")\n",
    "print(\"    5. Month\")\n",
    "print(\"    6. Day\")\n",
    "print(\"    7. Hour\")\n",
    "print(\"    8. Minute\")\n",
    "print(\"    9. Second\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  PROBLEMATIC FIELDS:\")\n",
    "print(\"  ‚Ä¢ Event ID: Missing in Morton (Dataset 3)\")\n",
    "print(\"  ‚Ä¢ Magnitude: Only in Littel (MAG) and Morton (Md) - missing in 3/5 datasets\")\n",
    "print(\"  ‚Ä¢ Num Stations: Different definitions across datasets\")\n",
    "print(\"  ‚Ä¢ Azimuthal Gap: Only in Brenton and Morton\")\n",
    "print(\"  ‚Ä¢ Error metrics: Different types (horizontal vs x/y, vertical vs z)\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDED UNIFIED SCHEMA:\")\n",
    "print(\"  REQUIRED FIELDS (all datasets):\")\n",
    "print(\"    ‚Ä¢ id (generate if missing)\")\n",
    "print(\"    ‚Ä¢ latitude\")\n",
    "print(\"    ‚Ä¢ longitude\")\n",
    "print(\"    ‚Ä¢ depth_km\")\n",
    "print(\"    ‚Ä¢ origin_time (construct from year/month/day/hour/minute/second)\")\n",
    "print(\"    ‚Ä¢ catalog_source (Brenton|Littel|Merrill|Morton|Shelly)\")\n",
    "print()\n",
    "print(\"  OPTIONAL FIELDS (null if missing):\")\n",
    "print(\"    ‚Ä¢ magnitude\")\n",
    "print(\"    ‚Ä¢ num_stations\")\n",
    "print(\"    ‚Ä¢ azimuthal_gap\")\n",
    "print(\"    ‚Ä¢ horizontal_error_km\")\n",
    "print(\"    ‚Ä¢ vertical_error_km\")\n",
    "print(\"    ‚Ä¢ rms_residual\")\n",
    "\n",
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"  1. ‚úÖ Push dataset_field_comparison.xlsx\")\n",
    "print(\"  2. Get approval on unified schema\")\n",
    "print(\"  3. Write transformation scripts to convert all 5 datasets to unified format\")\n",
    "print(\"  4. Load unified data into PostGIS database\")\n",
    "print(\"  5. Build catalog dropdown in frontend\")\n",
    "print(\"  6. Remove region-specific filters (W1/W2/E1/E2)\")\n",
    "print(\"  7. Add universal filters: lat/lon bounds, depth, date range, magnitude (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55434a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fcd057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 149\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"C:\\\\Users\\\\willi\\\\Downloads\\\\crescent_cfm_crustal_3d.geojson\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Number of features:\", len(data[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb9603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'MultiPolygon': 149})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "types = Counter(f[\"geometry\"][\"type\"] for f in data[\"features\"])\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 149 Counter({'MultiPolygon': 149})\n",
      "total vertices: 486456\n",
      "max vertices in a feature: 17472\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Define your path variable here\n",
    "path = \"C:\\\\Users\\\\willi\\\\Downloads\\\\crescent_cfm_crustal_3d.geojson\"\n",
    "\n",
    "# 2. Pass the 'path' variable into the open function\n",
    "with open(path, \"r\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "geom_types = Counter([ft[\"geometry\"][\"type\"] for ft in gj[\"features\"]])\n",
    "nfeat = len(gj[\"features\"])\n",
    "\n",
    "total_vertices = 0\n",
    "max_vertices = 0\n",
    "\n",
    "for ft in gj[\"features\"]:\n",
    "    coords = ft[\"geometry\"][\"coordinates\"]  # MultiPolygon\n",
    "    v = 0\n",
    "    for poly in coords:\n",
    "        for ring in poly:\n",
    "            v += len(ring)\n",
    "    total_vertices += v\n",
    "    max_vertices = max(max_vertices, v)\n",
    "\n",
    "print(\"features:\", nfeat, geom_types)\n",
    "print(\"total vertices:\", total_vertices)\n",
    "print(\"max vertices in a feature:\", max_vertices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ornl_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
